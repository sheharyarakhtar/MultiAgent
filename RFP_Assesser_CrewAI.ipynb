{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheharyarakhtar/MultiAgent/blob/main/RFP_Assesser_CrewAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "J6p5OfdKeuWb"
      },
      "outputs": [],
      "source": [
        "%pip -q install langchain langchain_community langchain_google_genai crewai crewai_tools faiss-gpu fastembed\n",
        "%pip -q install llama-index-embeddings-huggingface\n",
        "%pip -q install llama-index-embeddings-instructor\n",
        "%pip -q install -U duckduckgo-search\n",
        "%pip -q install langchain_huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b75bzo_-Ovt"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_core.vectorstores import VectorStoreRetriever\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import os\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai_tools import SerperDevTool, tool, PDFSearchTool\n",
        "import pathlib\n",
        "import textwrap\n",
        "import google.generativeai as genai\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from google.colab import userdata\n",
        "from langchain.tools import DuckDuckGoSearchRun\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "from IPython.display import display, Markdown, Latex"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple RAG (Retrieval Augmented Generator) using PDFs"
      ],
      "metadata": {
        "id": "hpT0D3cKXZI9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcxPnVzxlgJV",
        "outputId": "21586ca3-198f-47e2-e52f-a0db67997720"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Documents TENDER_49439_0_1724_1_1.pdf loaded successfully!\n",
            "Documents TENDER_49439_0_1724_1_2.pdf loaded successfully!\n",
            "Vector store created!\n",
            "Retriever created!\n"
          ]
        }
      ],
      "source": [
        "class RFPAssesser:\n",
        "    def __init__(self, model_name, embeddings_model_name):\n",
        "        self.llm = ChatGoogleGenerativeAI(model=model_name,\n",
        "                                          verbose=True,\n",
        "                                          google_api_key=userdata.get('GOOGLE_API_KEY'),\n",
        "                                          temperature = 0.3)\n",
        "        self.embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=embeddings_model_name,\n",
        "            show_progress=False\n",
        "            )\n",
        "        self.all_pages = []\n",
        "        self.vectorDB = None\n",
        "        self.retriever = None\n",
        "        self.assesser = None\n",
        "\n",
        "    def EmbedDocuments(self, paths):\n",
        "        for path in paths:\n",
        "            loader = PyPDFLoader(path)\n",
        "            pages = loader.load_and_split()\n",
        "            self.all_pages.extend(pages)\n",
        "            print(f\"Documents {path} loaded successfully!\")\n",
        "\n",
        "    def createVectorDB(self):\n",
        "      if not self.all_pages:\n",
        "          print(\"No documents loaded.\")\n",
        "          return\n",
        "      if not self.embeddings:\n",
        "          print(\"No embeddings provided.\")\n",
        "          return\n",
        "      embeddings = self.embeddings.embed_documents([page.page_content for page in self.all_pages])\n",
        "      if not embeddings:\n",
        "          print(\"Failed to generate embeddings.\")\n",
        "          return\n",
        "      self.vectorDB = FAISS.from_documents(self.all_pages, self.embeddings)\n",
        "      print(\"Vector store created!\")\n",
        "\n",
        "    def createRetriever(self):\n",
        "        self.retriever = VectorStoreRetriever(vectorstore=self.vectorDB)\n",
        "        self.assesser = RetrievalQA.from_chain_type(llm=self.llm, retriever=self.retriever)\n",
        "        print(\"Retriever created!\")\n",
        "\n",
        "    def setup(self, pdfs):\n",
        "        self.EmbedDocuments(paths=pdfs)\n",
        "        self.createVectorDB()\n",
        "        self.createRetriever()\n",
        "\n",
        "    def run(self, prompt):\n",
        "      return self.assesser.run(prompt)\n",
        "pdfs = [i for i in os.listdir() if 'pdf' in i]\n",
        "assesser = RFPAssesser(\n",
        "    model_name='gemini-1.5-flash',\n",
        "    embeddings_model_name=\"sentence-transformers/msmarco-distilbert-base-v4\"\n",
        "    )\n",
        "assesser.setup(pdfs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating tools to be used by LLM Agents"
      ],
      "metadata": {
        "id": "ezNZibjRXhes"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p7HVVTgsf9h"
      },
      "outputs": [],
      "source": [
        "@tool\n",
        "def Assesser(question: str, assesser = assesser) -> str:\n",
        "  \"\"\"\n",
        "  This tool allows you to query the provided documents.\n",
        "  You can ask it relevant questions and it will return the answer.\n",
        "  \"\"\"\n",
        "  prompt = f\"\"\"\"\n",
        "  Context: You are the owner of the RFP documents you have access to.\n",
        "  You will be asked questions regarding these documents.\n",
        "  You answer these questions by assessing the documents you have access to.\n",
        "\n",
        "  Question: {question}\n",
        "  \"\"\"\n",
        "  return assesser.run(prompt)\n",
        "\n",
        "@tool\n",
        "def DuckDuckGoSearch(search_query: str):\n",
        "    \"\"\"Search the web for information on a given topic\"\"\"\n",
        "\n",
        "    return DuckDuckGoSearchRun().run(search_query)\n",
        "\n",
        "serper_tool = SerperDevTool()\n",
        "\n",
        "import json\n",
        "\n",
        "@tool\n",
        "def SearchRFP(keywords: str, assesser = assesser) -> str:\n",
        "    \"\"\"\n",
        "    Search the RFP documents with keywords\n",
        "\n",
        "    keywords: str: Search keyword in the attached documents\n",
        "    conducts a similarity search of document and results a dictionary of relevant content with page numbers\n",
        "    \"\"\"\n",
        "    results = assesser.vectorDB.similarity_search(keywords)\n",
        "    overall = {}\n",
        "    content = \"\"\n",
        "    for result in results:\n",
        "      combine = {\n",
        "          'Page Number':result.metadata['page'],\n",
        "          'Content':result.page_content\n",
        "      }\n",
        "      overall[result.metadata['source']] = combine\n",
        "      content = content + result.page_content + \"\\n\\n\\n\"\n",
        "    prettyjson = json.dumps(overall, indent=4)\n",
        "    content = rephrase_text(content)\n",
        "    return content\n",
        "\n",
        "def rephrase_text(input_string, model = assesser.llm):\n",
        "  rephrased = model.invoke(\n",
        "        f\"\"\"\n",
        "        Context: You are a language model used to rephrase messy text.\n",
        "        You will be given some messy text and your job is to format it and paraphrase it.\n",
        "        Do not add anything yourself.\n",
        "        Remove things that are not relevant i.e. any tables or idle letters. Just rephrase the content in a single or max 2 paragraphs\n",
        "\n",
        "        Text: {input_string}\n",
        "        \"\"\").content\n",
        "  return rephrased\n",
        "\n",
        "@tool\n",
        "def write_to_file(input_string, file_path):\n",
        "    \"\"\"\n",
        "    Write input_string to a text file at file_path.\n",
        "\n",
        "    Parameters:\n",
        "    - input_string (str): The string to write to the file.\n",
        "    - file_path (str): The path where the file will be created or overwritten.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'w') as file:\n",
        "            file.write(input_string)\n",
        "        print(f\"Successfully wrote to file: {file_path}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing to file {file_path}: {e}\")\n",
        "\n",
        "\n",
        "@tool\n",
        "def pdf_search(query: str, pdf_path: str) -> str:\n",
        "  \"\"\"\n",
        "  Pass in the query you wish you ask and the path to the PDF you wish to search the query in\n",
        "  query: string\n",
        "  pdf_path: string\n",
        "  The function will return a string explaining what the pdf says about the query\n",
        "  \"\"\"\n",
        "\n",
        "  return rag.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LC9MoAEvvvOL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Agents and Tasks for Multi Agent Workflow"
      ],
      "metadata": {
        "id": "shgw10b5XmHq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "IJTa24vCc_lZ",
        "outputId": "c7d985ca-f006-4099-aff3-7e92322926e8"
      },
      "outputs": [
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for Agent\n  Value error, 1 validation error for CrewAgentExecutor\ntools -> 0\n  value is not a valid dict (type=type_error.dict) [type=value_error, input_value={'role': 'RFP Assesser', ...__main__.RFPAssesser'>]}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.8/v/value_error",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c714817b66c4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatGoogleGenerativeAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gemini-1.5-flash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgoogle_api_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GOOGLE_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'pdf'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m rfp_assesser = Agent(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mrole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'RFP Assesser'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgoal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"You are a Consultant in the {team_name} team. You need to ask relevant questions to the tool to assess if there are any use-cases related to you\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_ops_agent_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/crewai/agents/agent_builder/base_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__pydantic_self__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mmodel_validator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"after\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;31m# The following line sets a flag that we use to determine when `__init__` gets overridden by the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for Agent\n  Value error, 1 validation error for CrewAgentExecutor\ntools -> 0\n  value is not a valid dict (type=type_error.dict) [type=value_error, input_value={'role': 'RFP Assesser', ...__main__.RFPAssesser'>]}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.8/v/value_error"
          ]
        }
      ],
      "source": [
        "llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash', verbose=True,google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "pdfs = [i for i in os.listdir() if 'pdf' in i]\n",
        "rfp_assesser = Agent(\n",
        "    role = 'RFP Assesser',\n",
        "    goal=\"You are a Consultant in the {team_name} team. You need to ask relevant questions to the tool to assess if there are any use-cases related to you\",\n",
        "    backstory = (\n",
        "        \"You are a consultant in the {team_name} team\"\n",
        "        \"Clients come to you with their request for proposals\"\n",
        "        \"These documents contain their vision for their company, or problems they would like to solve\"\n",
        "        \"You are assigned a Assesser tool which you can use to interact with each pdf\"\n",
        "    ),\n",
        "    verbose = True,\n",
        "    memory = True,\n",
        "    llm = llm,\n",
        "    tools = [RFPAssesser]\n",
        ")\n",
        "summarise_usecases = Task(\n",
        "    description=(\n",
        "        \"Ask questions relevant to you from the data using the tools assigned. You need to find all cases relevant to you as part of the {team_name} team\"\n",
        "        \"The RFP does not answer all questions. Where the answers are unavailable, you make educated assumptions and move on.\"\n",
        "        \"You assess these documents to see where you, being part of the {team_name} team, can help solve some, if any, of these problems\"\n",
        "        ),\n",
        "    expected_output = \"A summary in markdown syntax detailing the use-cases that are relevant to your field, along with their page-numbers. Write these files using write_to_file tool\",\n",
        "    agent = rfp_assesser,\n",
        "    tools = [RFPAssesser]\n",
        ")\n",
        "crew = Crew(\n",
        "    agents = [rfp_assesser],\n",
        "    tasks = [summarise_usecases],\n",
        "    process = Process.sequential\n",
        ")\n",
        "crew.kickoff(inputs = {'team_name':'Data Science',\n",
        "                       'team_tools': 'Data Science',\n",
        "                       'pdfs': ', '.join(pdfs) })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "A10iNQGBumzV",
        "outputId": "29d33200-b67c-4788-999a-3eb620df04f8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'TENDER_49439_0_1724_1_1.pdf, TENDER_49439_0_1724_1_2.pdf'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdfs = [i for i in os.listdir() if 'pdf' in i]\n",
        "', '.join(pdfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XY_RjqsLnyZ2"
      },
      "outputs": [],
      "source": [
        "rfp_assesser = Agent(\n",
        "    role = 'RFP Assesser',\n",
        "    goal=\"You are a Consultant in the {team_name} team. You need to ask relevant questions to the tool to assess if there are any use-cases related to you\",\n",
        "    backstory = (\n",
        "\n",
        "        \"You are a consultant in the {team_name} team\"\n",
        "        \"Clients come to you with their request for proposals\"\n",
        "        \"These documents contain their vision for their company, or problems they would like to solve\"\n",
        "        \"You may ask relevant questions to the documents to assess its relevance to your field using the Assesser tool\"\n",
        "        \"You dont present a solution for a problem that the client has not explicitly asked you to solve\"\n",
        "        \"If there is no problem mentioned explicitly related to your field, you may use the Assesser tool to find which potential use-cases can be created\"\n",
        "        \"You also have direct access to the RFP using the SearchRFP tool which results content directly from the document in a string format\"\n",
        "        \"DO NOT ask for the entire RFP, only interact with it by asking questions and searching keywords\"\n",
        "        \"DO NOT repeat questions, only ask one question once and only search a single keyword once.\"\n",
        "        \"Ask a maximum of 2-3 questions\"\n",
        "    ),\n",
        "    verbose = True,\n",
        "    memory = True,\n",
        "    llm = assesser.llm,\n",
        "    tools = [Assesser,  write_to_file],\n",
        "    max_iterations = 5\n",
        "\n",
        ")\n",
        "summarise_usecases = Task(\n",
        "    description=(\n",
        "        \"Ask questions relevant to you from the data using the tools assigned. You need to find all cases relevant to you as part of the {team_name} team\"\n",
        "        \"The RFP does not answer all questions. Where the answers are unavailable, you make educated assumptions and move on.\"\n",
        "        \"You assess these documents to see where you, being part of the {team_name} team, can help solve some, if any, of these problems\"\n",
        "        ),\n",
        "    expected_output = \"A summary in markdown syntax detailing the use-cases that are relevant to your field, along with their page-numbers. Write these files using write_to_file tool\",\n",
        "    agent = agent,\n",
        "    tools = [Assesser, write_to_file]\n",
        ")\n",
        "\n",
        "researcher = Agent(\n",
        "    role = \"Research Assistant\",\n",
        "    goal = \"You will provide cutting edge solutions to the problems presented to you by RFP Assesser\",\n",
        "    backstory = (\n",
        "        \"Your coworker, RFP Assesser, has assessed the client's RFP and come up with potential places where {team_name} can be used\"\n",
        "        \"You have access to a search tool which you can use to find relevant literature online\"\n",
        "        \"You need to summarise each use-case into an achievable target\"\n",
        "        \"Then you will search online to find relevant literature that will help you orchestrate each of these solutions\"\n",
        "    ),\n",
        "    verbose = True,\n",
        "    memory = True,\n",
        "    llm = assesser.llm\n",
        ")\n",
        "\n",
        "specifying_task = Task(\n",
        "    description = (\n",
        "        \"You are provided with a holistic view of where the client wants to make improvements.\"\n",
        "        \"You need to {num_solutions} solutions from these potential use-cases\"\n",
        "        \"You need to be very specific about what problem is being solved and how its being solved\"\n",
        "    ),\n",
        "    expected_output = \"A title and a summary for each usecase presented, using the information gathered by your coworker RFP Assesser\",\n",
        "    agent = researcher\n",
        ")\n",
        "\n",
        "literature_review = Task(\n",
        "    description = (\n",
        "        \"Now that you have specific tasks, you have been given the tools to search online for relevant literature\"\n",
        "        \"Search whatever you think is relevant and support your use-cases with proper literature review\"\n",
        "    ),\n",
        "    expected_output = \"Title for each usecase with a paragraph and a URL to link each solution with relevant literature\",\n",
        "    agent = researcher,\n",
        "    tools = [DuckDuckGoSearch]\n",
        ")\n",
        "\n",
        "crew = Crew(\n",
        "    agents = [rfp_assesser, researcher],\n",
        "    tasks = [summarise_usecases, specifying_task, literature_review],\n",
        "    process = Process.sequential\n",
        ")\n",
        "result = crew.kickoff(inputs={'team_name': 'Data Science, AI and Machine Learning',\n",
        "                     'team_tools': 'Data Science, AI and ML',\n",
        "                              'num_solutions':'4'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y4d0O_u6rNEt",
        "outputId": "de5c75b9-39bf-4db0-ae06-52e6e1a81b24"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The provided text describes the requirements for bidders, not the client. It outlines the expectations for proposals, including team structure, documentation standards, and compliance with project scope. \n\nTo answer your question, we need more information about the specific project or client.  What is the context of this document? What is the project about? \n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The provided context doesn't mention any specific platform the client expects. \n\nTo recommend a platform, I need more information about the project. What kind of project is it? What are the specific requirements? \n\nOnce I have more context, I can suggest a suitable platform. \n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the provided RFP excerpts, here are some potential use cases that could be proposed to the client:\n\n**From Table 37 and Bid (Proposal) Response Requirement:**\n\n* **Resource Optimization and Forecasting:**  Analyze historical resource deployment data to predict future resource needs, optimize allocation, and identify potential bottlenecks. This could involve building models to forecast demand for specific roles based on project phases, skillsets, and project complexity.\n* **Team Performance Analysis:**  Track and analyze team performance metrics (e.g., time to completion, task efficiency, communication patterns) to identify areas for improvement and optimize team composition. This could involve using data visualization tools to identify trends and patterns in team performance.\n* **Skill Gap Analysis:**  Identify skill gaps within the team and recommend training or recruitment strategies to address them. This could involve building a skills matrix and comparing it to project requirements to identify areas where additional expertise is needed.\n* **Project Risk Assessment:**  Use historical data and project management methodologies to identify potential risks and develop mitigation strategies. This could involve building predictive models to assess the likelihood of project delays or cost overruns.\n\n**From Table 38:**\n\n* **Document Automation:**  Develop automated workflows for document creation, formatting, and approval processes. This could involve using natural language processing (NLP) techniques to extract information from existing documents and generate new ones.\n* **Document Analysis and Insights:**  Extract insights from project documents (e.g., reports, meeting minutes, emails) to identify trends, patterns, and potential areas of concern. This could involve using machine learning algorithms to analyze text data and identify key themes and topics.\n\n**From Table 30 and 31:**\n\n* **Project Performance Tracking and Reporting:**  Develop dashboards and reports to track project progress, identify potential issues, and provide insights into project performance. This could involve using data visualization tools to present key metrics and trends in an easily understandable format.\n* **GIS Data Analysis and Visualization:**  Analyze and visualize GIS data to identify patterns, trends, and insights. This could involve using spatial analysis techniques to understand the relationships between different geographic features and develop data-driven solutions.\n\n**General Use Cases:**\n\n* **Contract Management and Compliance:**  Develop systems to automate contract management processes, track compliance with contractual obligations, and identify potential risks.\n* **Data Governance and Security:**  Implement data governance policies and procedures to ensure data quality, security, and compliance with relevant regulations.\n\n**Important Considerations:**\n\n* **Data Availability:**  The success of these use cases will depend on the availability of relevant data. The RFP should be reviewed to determine what data is available and what data needs to be collected.\n* **Client Requirements:**  The specific use cases proposed should be tailored to the client's specific needs and priorities.\n* **Technical Feasibility:**  The technical feasibility of implementing these use cases should be assessed.\n\nBy carefully considering the client's needs and the available data, data science consultants can propose a range of valuable use cases that can help the client improve their project management, resource allocation, and decision-making processes. \n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the provided RFP, here are some potential data science, AI, or machine learning use cases that could be proposed to the client:\n\n**From the RFP:**\n\n* **Predictive Maintenance:** The RFP mentions \"Cloud Infrastructure Rollout / Readiness Report\" and \"Hardware Warranty / Support documents/Contracts\". This suggests the client is interested in managing their IT infrastructure effectively. Predictive maintenance models could be developed to anticipate hardware failures, optimize maintenance schedules, and reduce downtime.\n* **Resource Optimization:** The RFP emphasizes resource allocation and utilization, particularly for the \"Post-Implementation Period\". Machine learning algorithms could be used to analyze historical data on resource deployment and predict future needs, enabling more efficient allocation of personnel and resources.\n* **Change Management:** The RFP includes a RACI matrix for \"Service Change Management\". This indicates a focus on managing changes to the UMS application. Machine learning could be used to analyze past change requests, identify patterns, and predict potential impacts of future changes, improving the change management process.\n* **Data Quality and Integrity:** The RFP mentions \"Maintain and update Master data (while preserving the data authenticity)\". Data quality and integrity are crucial for any application. Machine learning techniques can be used to detect and correct data errors, ensuring the accuracy and reliability of the UMS application.\n\n**Recommendations:**\n\n* **User Behavior Analysis:**  The RFP mentions \"User Acceptance Test Reports\" and \"Training Reports\". Analyzing user behavior data could provide insights into user preferences, identify areas for improvement in the UMS application, and personalize user experiences.\n* **Fraud Detection:**  While not explicitly mentioned, fraud detection is a common concern in many applications. Machine learning models could be trained to identify suspicious activities and patterns, helping to prevent fraud and protect the UMS system.\n* **Natural Language Processing (NLP):**  The RFP mentions \"Knowledge Transfer (HOTO) Documents, Training Materials, User Manuals\". NLP techniques could be used to automate the creation of documentation, generate summaries of training materials, and provide intelligent search capabilities within the UMS application.\n* **Automated Reporting and Analytics:**  The RFP mentions various reports, including \"QA Report\", \"Go-Live Report\", and \"Hypercare report\". Machine learning could be used to automate the generation of these reports, providing insights and visualizations that are more comprehensive and actionable.\n\n**Overall:**\n\nThe RFP provides a good foundation for exploring data science and AI applications. By focusing on the client's needs for resource optimization, change management, data quality, and user experience, you can propose solutions that deliver real value and improve the effectiveness of the UMS application. \n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom MultiAgent workflow I created Myself in OOP"
      ],
      "metadata": {
        "id": "6cVAV_G8Irvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "ftuNlCRN3Yyl",
        "outputId": "a687f51e-8d77-4d2a-db5e-d3afe64fa067"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n",
            "WARNING:langchain_google_genai.chat_models:Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 Resource has been exhausted (e.g. check quota)..\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the information gathered, here are 6 specific use cases for data science, AI, and machine learning that could be beneficial for Muscat Municipality:\n\n**Usecase 1: Predictive Maintenance for Infrastructure**\n\n* **Brief Detail:** Analyze historical data from infrastructure assets (e.g., roads, buildings, utilities) to predict maintenance needs and optimize resource allocation. This can help prevent costly breakdowns and improve overall infrastructure lifespan.\n\n**Usecase 2: Smart Traffic Management**\n\n* **Brief Detail:** Utilize real-time traffic data from sensors, cameras, and mobile devices to optimize traffic flow, reduce congestion, and improve travel times. This can be achieved through AI-powered traffic signal control, dynamic route guidance, and parking availability prediction.\n\n**Usecase 3: Citizen Engagement and Feedback Analysis**\n\n* **Brief Detail:** Analyze data from social media, online surveys, and citizen feedback platforms to understand public sentiment, identify areas for improvement, and enhance citizen engagement. This can be achieved through natural language processing (NLP) and sentiment analysis techniques.\n\n**Usecase 4: Optimized Resource Allocation for Public Services**\n\n* **Brief Detail:** Analyze historical data on service requests, population demographics, and resource utilization to optimize the allocation of public services like waste management, street cleaning, and public safety. This can improve service efficiency and reduce costs.\n\n**Usecase 5:  Fraud Detection and Prevention in Municipal Services**\n\n* **Brief Detail:** Implement machine learning models to detect fraudulent activities in municipal services, such as building permits, tax payments, and public assistance programs. This can help protect public funds and ensure fair access to services.\n\n**Usecase 6:  Data-Driven Urban Planning and Development**\n\n* **Brief Detail:** Utilize geospatial data, demographic information, and historical trends to inform urban planning decisions, optimize land use, and promote sustainable development. This can help create more livable and efficient urban environments.\n\n**Question for the Client:**\n\n* **What are the specific data quality issues faced by the Muscat Municipality with its current data sources, particularly in relation to the Etamid portal, GIS Citizen Portal, and Building Permit Application?  How do these issues impact the effectiveness of these applications?** \n\nThis question will help understand the current data quality challenges and their impact on the municipality's digital initiatives. This information is crucial for designing effective data science solutions. \n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "questions = [\n",
        "    \"Can you give requirements of the client?\",\n",
        "    \"What platform does client expect us to use? If the client does not mention explicitly, can you recommend one?\",\n",
        "    ]\n",
        "# result = assesser.run(\"Can you give requirements of the client?\")\n",
        "# result = assesser.run(\"What platform does client expect us to use? If the client does not mention explicitly, can you recommend one?\")\n",
        "# result = assesser.run(\"\"\"\n",
        "# Assume you are a data science consultant assessing the RFP.\n",
        "# Create a list of all usecases that can be proposed to the client\n",
        "# based on what they have mentioned about their needs in the RFP.\"\"\")\n",
        "\n",
        "# result = assesser.run(\"\"\"\n",
        "# Assume you are a data science consultant assessing the RFP.\n",
        "# Create a list of all usecases that can be proposed to the client relating to data science, ai or machine learning\n",
        "# based on what they have mentioned about their needs in the RFP.\n",
        "# If there arent any usecases by the client, please recommend some\"\"\")\n",
        "class customAgent:\n",
        "  def __init__(self, llm, rag, max_iterations, goal, backstory):\n",
        "    self.rag = rag\n",
        "    self.llm = llm\n",
        "    self.max_iterations = max_iterations\n",
        "    self.goal = goal\n",
        "    self.backstory = backstory\n",
        "    self.previous_interactions = \"\"\n",
        "    self.prompt = None\n",
        "    self.task = None\n",
        "    self.question = None\n",
        "    self.answer = None\n",
        "    self.final_output = None\n",
        "  def ask_question(self):\n",
        "    for i in range(self.max_iterations):\n",
        "      self.prompt = f\"\"\"\n",
        "        You are an agent and you will be assigned a task.\n",
        "        Your backstory is: {self.backstory}\n",
        "        Your goal is: {self.goal}\n",
        "        Your previous interactions are: {self.previous_interactions}\n",
        "        You get to ask the client only {self.max_iterations} questions so be very specific and concise.\n",
        "      \"\"\"\n",
        "      self.task = \"Please respond with a single question\"\n",
        "      self.prompt = self.prompt+self.task\n",
        "      self.question = self.llm.invoke(self.prompt).content\n",
        "      # print(self.question)\n",
        "      self.answer = self.rag.run(self.question)\n",
        "      # print(self.answer)\n",
        "      # print(\"===========================================================================\")\n",
        "      self.previous_interactions += f\"Question: {self.question}\\nAnswer: {self.answer}\\n\\n\"\n",
        "    self.prompt = f\"\"\"\n",
        "        You are an agent and you will be assigned a task.\n",
        "        Your backstory is: {self.backstory}\n",
        "        Your goal is: {self.goal}\n",
        "        Your previous interactions are: {self.previous_interactions}\n",
        "        You get to ask the client only {self.max_iterations} questions so be very specific and concise.\n",
        "      \"\"\"\n",
        "    self.task = \"Please suggest your solution now based on your assessment of the client based on your interactions. Ask any relevant questions that can be forwarded to the client\"\n",
        "    self.prompt = self.prompt+self.task\n",
        "    self.final_output = self.llm.invoke(self.prompt).content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "agent = customAgent(\n",
        "    llm = assesser.llm,\n",
        "    rag = assesser,\n",
        "    max_iterations = 15,\n",
        "    goal = \"\"\"\n",
        "        Your goal is to find use cases related to data science, AI and machine learning.\n",
        "        Find atleast 6 usecases.\n",
        "        Do not be generic, be specific with your usecases.\n",
        "        Give your output in the format <Usecase Name>: <Brief Detail>\n",
        "        \"\"\",\n",
        "    backstory = \"\"\"\n",
        "    You are a data science consultant.\n",
        "    You have access to a tool which is a retrieval augmented generator\n",
        "    This tool has read the documents sent over by the client and acts as the owner of the PDFs\n",
        "    You can ask it relevant questions related to the document to determine where you can intervene.\n",
        "    You cannot ask the same question more than once.\n",
        "    \"\"\")\n",
        "agent.ask_question()\n",
        "display(Markdown(agent.final_output))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CwYxuZqkNJU9",
        "outputId": "eb6081c0-1a99-4e26-e96d-44e3881b83e1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question: What is the primary business domain or industry of the client? \n\nAnswer: The client is a **government entity or domain**. This is stated in the context provided. \n\n\nQuestion: What specific government agency or department does the client represent? \n\nAnswer: The client is the **Muscat Municipality**. \n\n\nQuestion: What are the primary goals and objectives of the Muscat Municipality? \n\nAnswer: The provided text focuses on the IT landscape of various municipalities in Oman, including Muscat Municipality. It doesn't explicitly state the primary goals and objectives of Muscat Municipality. \n\nTo find the primary goals and objectives of Muscat Municipality, you would need to consult their official website or other official documents. \n\n\nQuestion: What are the key challenges or problems that the Muscat Municipality is currently facing in relation to its IT infrastructure or data management? \n\nAnswer: The provided text doesn't explicitly state the challenges Muscat Municipality is facing with its IT infrastructure or data management. However, it does highlight some areas that could potentially lead to challenges:\n\n* **Legacy Systems:** The text mentions that Muscat Municipality uses a legacy Oracle E-business Suite (CRM Application) deployed in 2007. This could pose challenges in terms of:\n    * **Maintenance:** Maintaining and upgrading legacy systems can be costly and difficult.\n    * **Integration:** Integrating legacy systems with newer applications can be complex.\n    * **Security:** Legacy systems may have outdated security features, making them vulnerable to attacks.\n* **Data Silos:** The text mentions multiple applications and databases being used across different departments. This could lead to data silos, making it difficult to access and analyze data across the organization.\n* **Limited Integration:** While there are some integrations mentioned, the text suggests that not all applications are fully integrated. This could lead to inefficiencies and data inconsistencies.\n* **Mobile Application Limitations:** The Baladiyeti mobile application, while offering some services, is described as having limited functionality. This could hinder the municipality's ability to provide comprehensive digital services to citizens.\n\nIt's important to note that these are potential challenges based on the information provided. The text doesn't explicitly state that these are actual problems faced by Muscat Municipality. \n\n\nQuestion: What specific initiatives or projects is the Muscat Municipality currently undertaking related to digital transformation, smart city development, or citizen engagement? \n\nAnswer: Based on the provided context, the Muscat Municipality is undertaking the following initiatives related to digital transformation, smart city development, or citizen engagement:\n\n* **Etamid portal:** This GIS-enabled platform, developed in 2022, issues NOCs for excavation work. It aims to reduce turnaround time and improve coordination with external entities.\n* **GIS Citizen Portal:** This web-based GIS application allows citizens to view geospatial data within the Muscat Governorate. It also provides functionalities for internal users in departments like Building Permits, Lighting, Investments, and Technical Study.\n* **Building Permit Application:** This desktop-based application, built on ArcMap, provides tools for locating addresses, extracting information, and viewing data related to buildings, plots, and road networks. It is used by the GIS and Building Permits departments.\n\nThe context also mentions that the Muscat Municipality is currently working on a **Tender for Design, Implementation, Operation & Maintenance of Unified Municipal System**. This suggests a larger initiative to modernize and integrate its IT systems, potentially leading to further digital transformation and smart city development. \n\n\nQuestion: What specific data sources are currently being used by the Muscat Municipality for the Etamid portal, GIS Citizen Portal, and Building Permit Application? \n\nAnswer: The provided text doesn't explicitly state the specific data sources used for the Etamid portal, GIS Citizen Portal, and Building Permit Application in Muscat Municipality. \n\nIt does mention that Muscat Municipality has 70 GIS layers and uses general GIS applications for internal departments. However, it doesn't specify which data sources are used for each specific application. \n\n\nQuestion: What specific data is collected and used by the Muscat Municipality for the Etamid portal, GIS Citizen Portal, and Building Permit Application? \n\nAnswer: The provided text doesn't explicitly state the specific data collected and used by the Muscat Municipality for the Etamid portal, GIS Citizen Portal, and Building Permit Application. \n\nIt does mention that the Etamid portal is used to issue NOCs for excavation work, and that the GIS Citizen Portal allows citizens to view geospatial data. It also mentions that the Building Permit Application is used to locate addresses, extract information and data related to buildings, plots, and road networks. \n\nHowever, it doesn't specify the exact data points collected and used for each application. \n\n\nQuestion: What specific data is collected and used by the Muscat Municipality for the Etamid portal, GIS Citizen Portal, and Building Permit Application, including but not limited to:\n\n* **Etamid portal:**  What data is collected for NOC applications (e.g., applicant details, project details, location information, etc.)?\n* **GIS Citizen Portal:** What specific geospatial data is accessible to citizens (e.g., road networks, building permits, public facilities, etc.)?\n* **Building Permit Application:** What data is collected and used for building permit applications (e.g., property details, construction plans, building specifications, etc.)? \n\nAnswer: The provided text describes the functionality and purpose of the Etamid portal, GIS Citizen Portal, and Building Permit Application, but it doesn't explicitly mention the specific data collected and used by each application. \n\nTherefore, I cannot provide you with the exact data points collected for each application. \n\n\nQuestion: What are the specific data sources used for the Etamid portal, GIS Citizen Portal, and Building Permit Application, including the type of data (e.g., spatial data, demographic data, property data, etc.) and the format in which it is stored (e.g., databases, spreadsheets, GIS files, etc.)? \n\nAnswer: The provided text doesn't specify the exact data sources, types, and formats used for the Etamid portal, GIS Citizen Portal, and Building Permit Application. \n\nIt mentions that the Etamid portal is GIS-enabled and uses GIS layers, but it doesn't specify the data sources. Similarly, the GIS Citizen Portal is described as a web-based GIS application for viewing geospatial data, but it doesn't detail the data sources or formats. \n\nThe Building Permit Application is described as a desktop-based application built on ArcMap, which suggests it likely uses spatial data stored in GIS files or databases. However, the specific data sources and formats are not mentioned. \n\n\nQuestion: What are the specific challenges the Muscat Municipality faces in managing and integrating data across different departments and applications, particularly in relation to the Etamid portal, GIS Citizen Portal, and Building Permit Application? \n\nAnswer: The provided text doesn't explicitly mention specific challenges the Muscat Municipality faces in integrating data across the Etamid portal, GIS Citizen Portal, and Building Permit Application. \n\nHowever, it does highlight that Muscat Municipality has a diverse IT landscape with multiple applications and databases. This suggests potential challenges in:\n\n* **Data Silos:** Each application might have its own data storage, making it difficult to share and integrate information across different departments.\n* **Data Standardization:** Different applications might use different data formats and structures, making it challenging to combine data from multiple sources.\n* **Data Security and Access Control:** Ensuring data security and controlling access to sensitive information across different applications can be complex.\n* **Integration Complexity:** Integrating legacy systems with newer applications can be technically challenging and time-consuming.\n\nThe text mentions that the municipality is working on a unified platform to address these challenges. This platform aims to harmonize services and improve efficiency by leveraging GIS and smart solutions. \n\n\nQuestion: What are the specific data quality issues the Muscat Municipality faces with its current data sources, particularly in relation to the Etamid portal, GIS Citizen Portal, and Building Permit Application? \n\nAnswer: The provided text doesn't mention any specific data quality issues faced by Muscat Municipality regarding the Etamid portal, GIS Citizen Portal, or Building Permit Application. \n\n\nQuestion: What are the specific data quality issues the Muscat Municipality faces with its current data sources, particularly in relation to the Etamid portal, GIS Citizen Portal, and Building Permit Application, such as data accuracy, completeness, consistency, and timeliness? \n\nAnswer: The provided text doesn't explicitly mention data quality issues related to the Etamid portal, GIS Citizen Portal, and Building Permit Application in Muscat Municipality. \n\nIt focuses on the applications themselves, their functionalities, and the technologies used.  It doesn't delve into the specific challenges of data quality within these systems. \n\n\nQuestion: What are the specific data sources used for the Etamid portal, GIS Citizen Portal, and Building Permit Application, including the specific databases, file types, and data formats used for each? \n\nAnswer: The provided text does not specify the specific data sources, databases, file types, and data formats used for the Etamid portal, GIS Citizen Portal, and Building Permit Application. \n\n\nQuestion: What are the specific data quality metrics used by the Muscat Municipality to assess the accuracy, completeness, consistency, and timeliness of data used in the Etamid portal, GIS Citizen Portal, and Building Permit Application? \n\nAnswer: The provided text does not mention specific data quality metrics used by Muscat Municipality for the Etamid portal, GIS Citizen Portal, and Building Permit Application. \n\n\nQuestion: What are the specific data quality issues the Muscat Municipality faces with its current data sources, particularly in relation to the Etamid portal, GIS Citizen Portal, and Building Permit Application, and how do these issues impact the effectiveness of these applications? \n\nAnswer: The provided text doesn't explicitly mention specific data quality issues faced by Muscat Municipality.  It describes the applications and their functionalities but doesn't detail any challenges related to data quality. \n\n\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(agent.previous_interactions))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYt1BoJuMUflNMR9PL3Ptd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}